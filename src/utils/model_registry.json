{
  "description": "Model pairs for multi-agent alignment experiments (base + instruct)",
  "version": "1.0",
  "models": {
    "Mistral_7B": {
      "base": "mistralai/Mistral-7B-v0.3",
      "instruct": "mistralai/Mistral-7B-Instruct-v0.3",
      "display_name": "Mistral-7B",
      "hidden_dim": 4096,
      "num_layers": 32
    },
    "Llama3_8B": {
      "base": "meta-llama/Llama-3.1-8B",
      "instruct": "meta-llama/Llama-3.1-8B-Instruct",
      "display_name": "Llama-3.1-8B",
      "hidden_dim": 4096,
      "num_layers": 32
    },
    "Qwen2_7B": {
      "base": "Qwen/Qwen2-7B",
      "instruct": "Qwen/Qwen2-7B-Instruct",
      "display_name": "Qwen2-7B",
      "hidden_dim": 3584,
      "num_layers": 28
    },
    "Gemma2_9B": {
      "base": "google/gemma-2-9b",
      "instruct": "google/gemma-2-9b-it",
      "display_name": "Gemma-2-9B",
      "hidden_dim": 3584,
      "num_layers": 42
    },
    "Zephyr_7B": {
      "base": "mistralai/Mistral-7B-v0.1",
      "instruct": "HuggingFaceH4/zephyr-7b-beta",
      "display_name": "Zephyr-7B",
      "hidden_dim": 4096,
      "num_layers": 32,
      "note": "Zephyr is fine-tuned from Mistral v0.1"
    },
    "Falcon_7B": {
      "base": "tiiuae/falcon-7b",
      "instruct": "tiiuae/falcon-7b-instruct",
      "display_name": "Falcon-7B",
      "hidden_dim": 4544,
      "num_layers": 32
    }
  }
}
